# -*- coding: utf-8 -*-
"""Logistic Regression.ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/14fqbevnkVwpFjkTOm4QoAnWxI5FN81j_
"""

from sklearn.datasets import load_breast_cancer
import numpy as np

data = load_breast_cancer()
X,y = data.data, data.target
print(X.shape,y.shape)

from sklearn.preprocessing import StandardScaler
sc = StandardScaler()
X = sc.fit_transform(X)

from sklearn.model_selection import train_test_split
train_X, test_X, train_y, test_y = train_test_split(X,y,test_size=0.2,random_state = 0)
test_X, val_X, test_y, val_y = train_test_split(test_X,test_y,test_size=0.5,random_state = 0)
print(train_X.shape, val_X.shape, test_X.shape)

def sigmoid(z):
  return 1/(1 + np.exp(-z))

epochs = 1000
lr = 0.01
lambda1 = 0.1
lambda2 = 0.1
w = np.zeros(X.shape[1])
b = 0
epsilon = 1e-9
for epoch in range(epochs):
  z = train_X@w + b
  train_y_pred = sigmoid(z)
  loss = -np.mean(train_y * np.log(train_y_pred+epsilon) + (1-train_y) * (np.log(1-train_y_pred+epsilon)))
  regularisation = lambda1*np.sum(np.absolute(w)) + lambda2*np.sum(w**2)
  J = loss + regularisation
  dJ_dz = z*(1-z)
  dJ_dw = (1/(train_X.shape[0])) * train_X.T@(train_y_pred - train_y) + lambda1*np.sign(w) + 2*lambda2*w
  dJ_db = np.mean(train_y_pred - train_y)
  w -= lr*dJ_dw
  b -= lr*dJ_db
  if (epoch+1)%100==0:
    z = val_X@w + b
    val_y_pred = sigmoid(z)
    loss = -np.mean(val_y*np.log(val_y_pred+epsilon) + (1-val_y)*(np.log(1-val_y_pred+epsilon)))
    regularisation = lambda1*np.sum(np.absolute(w)) + lambda2*np.sum(w**2)
    J = loss + regularisation
    print(f"Epoch {epoch+1}: validation loss = {J}")

z = test_X @ w + b
test_y_pred = (sigmoid(z) >= 0.55)
acc = np.mean(test_y_pred == test_y)
print(f"Accuracy : {round(acc*100,2)}%")
tp = np.sum((test_y_pred == 1) & (test_y == 1))
tn = np.sum((test_y_pred == 0) & (test_y == 0))
fp = np.sum((test_y_pred == 1) & (test_y == 0))
fn = np.sum((test_y_pred == 0) & (test_y == 1))
print(f'''Confusion Matrix:\n
true positive = {tp}
true negative = {tn}
false positive = {fp}
false negative = {fn}\n''')
precision = tp/(tp+fp)
recall = tp/(tp+fn)
print(f"Precision = {round(precision,2)} and recall = {recall}")