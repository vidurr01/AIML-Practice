# -*- coding: utf-8 -*-
"""MLP (multi layer perceptron).ipynb

Automatically generated by Colab.

Original file is located at
    https://colab.research.google.com/drive/1q8MugeKjDepDV2Q6pN0ejgwjciExf0lN

DATASET AND FEATURE SCALING
"""

from sklearn.datasets import load_diabetes
from sklearn.preprocessing import StandardScaler

# Loading and diving into input and output
data = load_diabetes()
X, y = data.data, data.target

#Scaling for Gradient Descent to converge quickly.
scaler = StandardScaler()
X = scaler.fit_transform(X)
y = y.reshape(-1,1)

"""BREAK INTO TRAIN, VALIDATION AND TEST DATA"""

from sklearn.model_selection import train_test_split
train_X, test_X, train_y, test_y =  train_test_split(X,y,test_size=0.4, random_state = 0)
test_X, val_X, test_y, val_y =  train_test_split(test_X,test_y,test_size=0.5, random_state = 0)
print(train_X.shape, val_X.shape, test_X.shape)

"""CREATING RELU FUNCTION AND ITS DIFFERENTIATION"""

def relu(a):
  return np.maximum(0,a)
def drelu(a):
  return (a>0) # if element > 0, then 1 ; else 0

"""TRAINING AND VALIDATING MODEL"""

import numpy as np

epochs = 1500
lr = 1e-4
w1 = 0.01*np.random.randn(X.shape[1],32) # mistake 1: had used np.zeros instead
b1 = 0.01*np.random.randn(1,32)
w2 = 0.01*np.random.randn(w1.shape[1],1)
b2 = 0.01*np.random.randn(1,1)

for epoch in range(epochs):
  z1 = train_X@w1 + b1
  a1 = relu(z1)
  z2 = a1@w2 + b2
  a2 = z2
  train_J = np.mean((a2 - train_y)**2)

  dJ_da2 = (2 / train_X.shape[0]) * (a2 - train_y)
  dJ_dw2 = a1.T@dJ_da2
  dJ_db2 = np.sum(dJ_da2,axis=0)
  dJ_da1 = dJ_da2@w2.T
  dJ_dz1 = dJ_da1 * drelu(z1)
  dJ_dw1 = train_X.T@dJ_dz1
  dJ_db1 = np.sum(dJ_dz1,axis=0)

  w2 -= lr*dJ_dw2
  w1 -= lr*dJ_dw1
  b2 -= lr*dJ_db2
  b1 -= lr*dJ_db1

  if (epoch+1)%1000==0:
    z1 = val_X@w1 + b1
    a1 = relu(z1)
    z2 = a1@w2 + b2
    a2 = z2
    val_J = np.mean((a2 - val_y)**2)
    print(f"Epoch {epoch+1}: Training loss = {train_J}, Validation loss = {val_J}, difference = {val_J-train_J}")
print(f"baseline = {np.mean((val_y - np.mean(train_y))**2)}")
# now since our training loss is lesser than validation loss, it is overfitting. Since our baseline is higher than the losses, that means our model is learning and its not just noise.

"""TESTING MODEL"""

z1 = test_X@w1 + b1
a1 = relu(z1)
z2 = a1@w2 + b2
a2 = z2
J = np.mean((a2 - test_y)**2)
print(f"Testing loss = {round(J,2)}")

